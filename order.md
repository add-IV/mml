# Order of topics

- paper
- nlp tasks
- embeddings
  - cosine similarity
  - Superposition

- Attention

-----

- Multi-Head Attention
- Cross-Attention
- complete transformer model


- transformer variants
  - other attention mechanisms
  - other applications of transformers (e.g. stable diffusion)

- comparison to other models
  - RNN, LSTM, GRU, CNN

- Optimizations
  - Output Matrix Splitting
  - reuse of weights
  - ease of training (parallelization)

- other considerations
  - Masking
  - Positional Encoding
  - Context Size and its implications