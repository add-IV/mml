# Order of topics

Frederick

- Multi-Head Attention
- Cross-Attention
- complete transformer model

- transformer variants
  - other attention mechanisms
  - other applications of transformers (e.g. stable diffusion)

-----

Other topics

- comparison to other models
  - RNN, LSTM, GRU, CNN

- Optimizations
  - Output Matrix Splitting
  - reuse of weights
  - ease of training (parallelization)

- other considerations
  - Masking
  - Positional Encoding
  - Context Size and its implications
